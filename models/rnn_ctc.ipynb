{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem - 1: Speech to Text Detection (Automatic Speech Recognition)\n",
        "**Goal**\n",
        "\n",
        "Develop a system that converts spoken language into written text by processing audio signals. The goal is to accurately transcribe speech in real-time or from recorded audio.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Description\n",
        "\n",
        "- **Training Set:**  \n",
        "  - **LibriSpeech train-clean-100**  \n",
        "  - Contains ~100 hours of clean English speech from public domain audiobooks (LibriVox).  \n",
        "  - High-quality recordings with minimal background noise.  \n",
        "  - Suitable for training ASR models.  \n",
        "\n",
        "- **Validation Set:**  \n",
        "  - **LibriSpeech dev-clean**  \n",
        "  - Clean, studio-quality English speech data used for validation.  \n",
        "  - Helps monitor model performance during training and avoid overfitting.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2zK1aQk_Gkn",
        "outputId": "adacb1e1-bc4c-4d65-ce2c-30222ff2f37d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.2.1)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m120.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
            "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jiwer, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed jiwer-4.0.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 rapidfuzz-3.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchaudio jiwer matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Speech Recognition Setup\n",
        "\n",
        "- Imports libraries for audio processing, modeling, and evaluation.\n",
        "- Uses GPU if available, otherwise CPU.\n",
        "- Defines character vocabulary and mappings for text-to-index conversion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bcQsv8OU_LmP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchaudio.datasets import LIBRISPEECH\n",
        "from torchaudio.transforms import MelSpectrogram\n",
        "from jiwer import wer\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "CHAR_VOCAB = ['<blank>'] + list(\"abcdefghijklmnopqrstuvwxyz '\")\n",
        "CHAR2IDX = {c: i for i, c in enumerate(CHAR_VOCAB)}\n",
        "IDX2CHAR = {i: c for c, i in CHAR2IDX.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text and Decoding Functions\n",
        "\n",
        "- `text_to_indices(text)`: Converts text to a list of character indices for model input.\n",
        "\n",
        "- `greedy_decode(log_probs)`: Performs greedy decoding of model outputs, removes blanks and repeats, and returns final transcripts as text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZlViR1pa_qzI"
      },
      "outputs": [],
      "source": [
        "def text_to_indices(text):\n",
        "    return [CHAR2IDX[c] for c in text.lower() if c in CHAR2IDX]\n",
        "\n",
        "def greedy_decode(log_probs):\n",
        "    best_path = torch.argmax(log_probs, dim=-1)\n",
        "    transcripts = []\n",
        "    for seq in best_path:\n",
        "        prev = None\n",
        "        tokens = []\n",
        "        for idx in seq:\n",
        "            idx = idx.item()\n",
        "            if idx != prev and idx != 0:\n",
        "                tokens.append(IDX2CHAR[idx])\n",
        "            prev = idx\n",
        "        transcripts.append(\"\".join(tokens))\n",
        "    return transcripts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Extraction and Data Collation\n",
        "\n",
        "- `mel_transform`: Converts audio waveforms to Mel Spectrograms (80 filter banks, 16kHz sample rate).\n",
        "- `collate_fn(batch)`: Prepares batches by extracting features, converting transcripts to indices, padding sequences, and moving data to the correct device.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvnQS6bl_u4l"
      },
      "outputs": [],
      "source": [
        "mel_transform = MelSpectrogram(\n",
        "    sample_rate=16000, n_fft=400, win_length=400,\n",
        "    hop_length=160, n_mels=80\n",
        ")\n",
        "\n",
        "def collate_fn(batch):\n",
        "    features, targets, input_lengths, target_lengths = [], [], [], []\n",
        "    for waveform, sample_rate, transcript, speaker_id, chapter_id, utterance_id in batch:\n",
        "        mel = mel_transform(waveform).squeeze(0).transpose(0, 1)\n",
        "        target = torch.tensor(text_to_indices(transcript))  \n",
        "        features.append(mel)\n",
        "        targets.append(target)\n",
        "        input_lengths.append(mel.size(0))\n",
        "        target_lengths.append(len(target))\n",
        "    features = nn.utils.rnn.pad_sequence(features, batch_first=True)\n",
        "    targets = torch.cat(targets)\n",
        "    return (\n",
        "        features.to(DEVICE),\n",
        "        targets.to(DEVICE),\n",
        "        torch.tensor(input_lengths).to(DEVICE),\n",
        "        torch.tensor(target_lengths).to(DEVICE)\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Preparation\n",
        "\n",
        "- Loads the LibriSpeech dataset for training (`train-clean-100`), validation (`dev-clean`), and testing (`test-clean`).\n",
        "- Automatically downloads datasets if not already present.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRazgz2-_zRr",
        "outputId": "2c68aa7c-7ace-46a5-d031-2611f3aa4b08"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5.95G/5.95G [06:50<00:00, 15.6MB/s]\n",
            "100%|██████████| 322M/322M [00:16<00:00, 20.7MB/s]\n",
            "100%|██████████| 331M/331M [00:17<00:00, 19.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "from torchaudio.datasets import LIBRISPEECH\n",
        "\n",
        "train_dataset = LIBRISPEECH(\".\", url=\"train-clean-100\", download=True)\n",
        "\n",
        "val_dataset = LIBRISPEECH(\".\", url=\"dev-clean\", download=True)\n",
        "\n",
        "test_dataset = LIBRISPEECH(\".\", url=\"test-clean\", download=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DataLoader Setup\n",
        "\n",
        "- Creates DataLoaders for training, validation, and testing.\n",
        "- Uses `collate_fn` to process batches.\n",
        "- Training loader shuffles data; validation and test loaders do not.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "36rWD39aA5mH"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SpeechRNNCTC Class\n",
        "\n",
        "A simple speech recognition model using a bidirectional LSTM and linear layer, designed for use with CTC (Connectionist Temporal Classification) loss.\n",
        "\n",
        "\n",
        "**Initialization**\n",
        "\n",
        "```python\n",
        "SpeechRNNCTC(input_dim=80, hidden_dim=512, output_dim=len(CHAR_VOCAB))\n",
        "```\n",
        "\n",
        "**Arguments:**\n",
        "\n",
        "- `input_dim` (int): Size of input features per time step (default: `80`).\n",
        "- `hidden_dim` (int): Number of hidden units in the LSTM layers (default: `512`).\n",
        "- `output_dim` (int): Number of output classes, usually length of the character vocabulary (default: `len(CHAR_VOCAB)`).\n",
        "\n",
        "**Components**\n",
        "\n",
        "- `rnn`: 3-layer bidirectional LSTM for sequence modeling.\n",
        "- `fc`: Linear layer mapping LSTM outputs to output classes.\n",
        "\n",
        "**Forward Pass**\n",
        "\n",
        "```python\n",
        "output = model(x)\n",
        "```\n",
        "\n",
        "**Arguments:**\n",
        "\n",
        "- `x` (Tensor): Input tensor of shape `(batch_size, sequence_length, input_dim)`.\n",
        "\n",
        "**Returns:**\n",
        "\n",
        "- Tensor of shape `(batch_size, sequence_length, output_dim)` with class scores for each time step.\n",
        "\n",
        "**Notes**\n",
        "\n",
        "- Suitable for speech recognition with CTC loss where input-output alignment is unknown.\n",
        "- Make sure `CHAR_VOCAB` is defined as your character set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SHrPcT8CBciM"
      },
      "outputs": [],
      "source": [
        "class SpeechRNNCTC(nn.Module):\n",
        "    def __init__(self, input_dim=80, hidden_dim=512, output_dim=len(CHAR_VOCAB)):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers=3,\n",
        "                           bidirectional=True, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _ = self.rnn(x)\n",
        "        return self.fc(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training and Evaluation Function\n",
        "\n",
        "- `train_and_evaluate`: Trains and evaluates the model.\n",
        "- Supports different optimizers (`SGD`, `Adam`, `AdamW`, `RMSprop`) and schedulers (`StepLR`, `ReduceLROnPlateau`).\n",
        "- Tracks training/validation loss and epoch times.\n",
        "- Includes gradient clipping and device handling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaKhKPC3B_zS"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
        "\n",
        "def train_and_evaluate(\n",
        "    model_fn, train_dataset, val_dataset, loss_fn,\n",
        "    optimizer_name=\"AdamW\", batch_size=4, epochs=15,\n",
        "    device=\"cuda\", scheduler_type=\"step\", early_stopping=None\n",
        "):\n",
        "    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model_fn().to(device)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    optimizer = {\n",
        "        \"SGD\": lambda: torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9),\n",
        "        \"Adam\": lambda: torch.optim.Adam(model.parameters(), lr=0.001),\n",
        "        \"AdamW\": lambda: torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4),\n",
        "        \"RMSprop\": lambda: torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
        "    }.get(optimizer_name, None)\n",
        "\n",
        "    if optimizer is None:\n",
        "        raise ValueError(\"Unsupported optimizer\")\n",
        "\n",
        "    optimizer = optimizer()\n",
        "\n",
        "    scheduler = {\n",
        "        \"plateau\": ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2),\n",
        "        \"step\": StepLR(optimizer, step_size=5, gamma=0.1),\n",
        "        None: None\n",
        "    }.get(scheduler_type)\n",
        "\n",
        "    history = {\n",
        "        \"train_loss\": [], \"val_loss\": [], \"epoch_time\": []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start = time.time()\n",
        "\n",
        "        model.train()\n",
        "        total_train_loss = 0.0\n",
        "        for inputs, labels, input_lengths, target_lengths in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            input_lengths = input_lengths.to(device)\n",
        "            target_lengths = target_lengths.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)  \n",
        "            log_probs = F.log_softmax(outputs, dim=2)\n",
        "\n",
        "            loss = loss_fn(log_probs.transpose(0, 1), labels, input_lengths, target_lengths)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        total_val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels, input_lengths, target_lengths in val_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                input_lengths = input_lengths.to(device)\n",
        "                target_lengths = target_lengths.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                log_probs = F.log_softmax(outputs, dim=2)\n",
        "                val_loss = loss_fn(log_probs.transpose(0, 1), labels, input_lengths, target_lengths)\n",
        "                total_val_loss += val_loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1:02d} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Time: {time.time() - start:.2f}s\")\n",
        "\n",
        "        history[\"train_loss\"].append(avg_train_loss)\n",
        "        history[\"val_loss\"].append(avg_val_loss)\n",
        "        history[\"epoch_time\"].append(time.time() - start)\n",
        "\n",
        "        if scheduler_type == \"plateau\":\n",
        "            scheduler.step(avg_val_loss)\n",
        "        elif scheduler_type == \"step\":\n",
        "            scheduler.step()\n",
        "\n",
        "\n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Training\n",
        "\n",
        "- Defines CTC loss with blank index 0.\n",
        "- Initializes and trains `SpeechRNNCTC` model using `train_and_evaluate`.\n",
        "- Uses AdamW optimizer, batch size 8, for 15 epochs on GPU (if available).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcSvRj0uCJvl",
        "outputId": "d32f0207-f458-4850-a195-74a5e9966f4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01 | Train Loss: 1.3977 | Val Loss: 1.0376 | Time: 1204.93s\n",
            "Epoch 02 | Train Loss: 0.7843 | Val Loss: 0.8200 | Time: 1203.31s\n",
            "Epoch 03 | Train Loss: 0.6180 | Val Loss: 0.7376 | Time: 1203.32s\n",
            "Epoch 04 | Train Loss: 0.5215 | Val Loss: 0.6744 | Time: 1201.83s\n",
            "Epoch 05 | Train Loss: 0.4540 | Val Loss: 0.6372 | Time: 1203.81s\n",
            "Epoch 06 | Train Loss: 0.3180 | Val Loss: 0.5449 | Time: 1203.08s\n",
            "Epoch 07 | Train Loss: 0.2608 | Val Loss: 0.5345 | Time: 1203.76s\n",
            "Epoch 08 | Train Loss: 0.2265 | Val Loss: 0.5384 | Time: 1202.42s\n",
            "Epoch 09 | Train Loss: 0.1994 | Val Loss: 0.5451 | Time: 1203.68s\n",
            "Epoch 10 | Train Loss: 0.1758 | Val Loss: 0.5552 | Time: 1202.76s\n",
            "Epoch 11 | Train Loss: 0.1479 | Val Loss: 0.5594 | Time: 1202.54s\n",
            "Epoch 12 | Train Loss: 0.1425 | Val Loss: 0.5630 | Time: 1204.59s\n",
            "Epoch 13 | Train Loss: 0.1387 | Val Loss: 0.5660 | Time: 1202.15s\n",
            "Epoch 14 | Train Loss: 0.1353 | Val Loss: 0.5692 | Time: 1204.84s\n",
            "Epoch 15 | Train Loss: 0.1322 | Val Loss: 0.5726 | Time: 1204.30s\n"
          ]
        }
      ],
      "source": [
        "from torch.nn import CTCLoss\n",
        "\n",
        "loss_fn = CTCLoss(blank=0, zero_infinity=True)\n",
        "\n",
        "model_fn = lambda: SpeechRNNCTC()  \n",
        "trained_model, training_history = train_and_evaluate(\n",
        "    model_fn=model_fn,\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    loss_fn=loss_fn,\n",
        "    optimizer_name=\"AdamW\",\n",
        "    batch_size=8,\n",
        "    epochs=15,\n",
        "    device=\"cuda\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plotting Training History\n",
        "\n",
        "- `plot_training_history`: Plots training and validation loss curves over epochs.\n",
        "- Helps visualize model performance during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXjGoUpfChhI"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_history(history):\n",
        "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\", marker='o')\n",
        "    plt.plot(epochs, history[\"val_loss\"], label=\"Val Loss\", marker='x')\n",
        "\n",
        "    plt.title(\"Training vs Validation Loss\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"CTC Loss\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Evaluation on Test Data\n",
        "\n",
        "- `evaluate_model_on_test`: Evaluates model using Word Error Rate (WER) and Character Error Rate (CER).\n",
        "- Displays example predictions and computes average WER/CER.\n",
        "- Supports limiting number of batches and showing sample outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33OnnQBMDcSE"
      },
      "outputs": [],
      "source": [
        "from jiwer import wer, cer\n",
        "\n",
        "def evaluate_model_on_test(\n",
        "    model, test_loader, idx2char, device=\"cuda\", max_batches=5, show_samples=True\n",
        "):\n",
        "    model.eval()\n",
        "    total_wer, total_cer = 0.0, 0.0\n",
        "    total_samples = 0\n",
        "    samples_shown = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets, input_lengths, target_lengths) in enumerate(test_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            input_lengths = input_lengths.to(device)\n",
        "            target_lengths = target_lengths.to(device)\n",
        "\n",
        "            logits = model(inputs)\n",
        "            log_probs = torch.nn.functional.log_softmax(logits, dim=2)\n",
        "            pred_texts = greedy_decode(log_probs)\n",
        "\n",
        "            true_texts = []\n",
        "            idx = 0\n",
        "            for length in target_lengths:\n",
        "                text = \"\".join([idx2char[i.item()] for i in targets[idx:idx + length]])\n",
        "                true_texts.append(text)\n",
        "                idx += length\n",
        "\n",
        "            for ref, hyp in zip(true_texts, pred_texts):\n",
        "                total_wer += wer(ref, hyp)\n",
        "                total_cer += cer(ref, hyp)\n",
        "                total_samples += 1\n",
        "\n",
        "                if show_samples and samples_shown < 5:\n",
        "                    print(f\"REF: {ref}\")\n",
        "                    print(f\"HYP: {hyp}\")\n",
        "                    print(f\"WER: {wer(ref, hyp):.2f}, CER: {cer(ref, hyp):.2f}\")\n",
        "                    print(\"-\" * 60)\n",
        "                    samples_shown += 1\n",
        "\n",
        "            if max_batches is not None and (batch_idx + 1) >= max_batches:\n",
        "                break\n",
        "\n",
        "    avg_wer = total_wer / total_samples if total_samples > 0 else float(\"inf\")\n",
        "    avg_cer = total_cer / total_samples if total_samples > 0 else float(\"inf\")\n",
        "    print(f\"\\nAverage WER: {avg_wer:.4f}\")\n",
        "    print(f\"Average CER: {avg_cer:.4f}\")\n",
        "    return avg_wer, avg_cer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Final Model Evaluation\n",
        "\n",
        "- Evaluates the trained model on test data.\n",
        "- Calculates average WER and CER over 10 test batches.\n",
        "- Displays example transcriptions for comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIqIY_OODewo",
        "outputId": "e1903c68-4915-45d0-b77b-ff11b6628ddf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "REF: he hoped there would be stew for dinner turnips and carrots and bruised potatoes and fat mutton pieces to be ladled out in thick peppered flour fattened sauce\n",
            "HYP: he hoped there would be sto for dinner turnips and carrats and brused patatoes and fatn button pieces to be ladleld out ind thic peppered flower fatened souc\n",
            "WER: 0.43, CER: 0.09\n",
            "------------------------------------------------------------\n",
            "REF: stuff it into you his belly counselled him\n",
            "HYP: stuf id into you his belay councteled him\n",
            "WER: 0.50, CER: 0.14\n",
            "------------------------------------------------------------\n",
            "REF: after early nightfall the yellow lamps would light up here and there the squalid quarter of the brothels\n",
            "HYP: after early night fall the yenow lampse would light hap here and there the squalled quartter of the broawfals\n",
            "WER: 0.44, CER: 0.12\n",
            "------------------------------------------------------------\n",
            "REF: hello bertie any good in your mind\n",
            "HYP: hel iburty and e good in her mind\n",
            "WER: 0.71, CER: 0.35\n",
            "------------------------------------------------------------\n",
            "REF: number ten fresh nelly is waiting on you good night husband\n",
            "HYP: dum but den fresh nolyas waiting on you could noght husband\n",
            "WER: 0.64, CER: 0.22\n",
            "------------------------------------------------------------\n",
            "\n",
            "✅ Average WER: 0.2721\n",
            "✅ Average CER: 0.0834\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.2720786615312261, 0.08343493770278568)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate_model_on_test(\n",
        "    model=trained_model,\n",
        "    test_loader=test_loader,\n",
        "    idx2char=IDX2CHAR,\n",
        "    device=\"cuda\",\n",
        "    max_batches=10  \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save the Trained Model\n",
        "\n",
        "- Saves the model's learned parameters to a `.pth` file for future use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXwKRAfAT2PL"
      },
      "outputs": [],
      "source": [
        "torch.save(trained_model.state_dict(), \"speech_to_text_model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training Results\n",
        "\n",
        "- Total Epochs: **15**\n",
        "- Final Training Loss: **0.1322**\n",
        "- Final Validation Loss: **0.5726**\n",
        "- Average Epoch Time: ~1203 seconds\n",
        "\n",
        "\n",
        "### Test Set Evaluation\n",
        "\n",
        "**What is WER and CER?**\n",
        "\n",
        "- **WER (Word Error Rate):**  \n",
        "  Measures the percentage of words incorrectly predicted, calculated as:  \n",
        "  `(Substitutions + Insertions + Deletions) / Total Words`  \n",
        "  Lower WER means better transcription accuracy.\n",
        "\n",
        "- **CER (Character Error Rate):**  \n",
        "  Similar to WER but at the character level, calculated as:  \n",
        "  `(Substitutions + Insertions + Deletions) / Total Characters`  \n",
        "  Lower CER indicates fewer character-level mistakes.\n",
        "\n",
        "\n",
        "**Observed Test Results**\n",
        "\n",
        "- WER ranges from **0.43 to 0.71** on individual examples.\n",
        "- CER ranges from **0.09 to 0.35** on individual examples.\n",
        "- Common errors include substitutions, deletions, or character-level mistakes.\n",
        "\n",
        "\n",
        "**Performance Summary**\n",
        "\n",
        "| Metric | Your Model | Ideal Range (Good Models) |\n",
        "|--------|------------|---------------------------|\n",
        "| **WER** | 0.2721 | ≤ 0.15 (Excellent), ≤ 0.25 (Good) |\n",
        "| **CER** | 0.0834 | ≤ 0.05 (Excellent), ≤ 0.10 (Good) |\n",
        "\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "- **CER** is within a reasonable range, showing good character-level accuracy.\n",
        "- **WER** indicates room for improvement at the word level.\n",
        "- Future improvements may include more training data, larger models, or better decoding techniques.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
