{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Translation using Transformer Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Dataset Description\n",
    "- **Dataset**: IWSLT2017 English-German translation dataset.\n",
    "- **Purpose**: Benchmarking translation models on small-scale datasets.\n",
    "- **Content**: Parallel English-German sentence pairs for machine translation tasks.\n",
    "- **Source**: Hugging Face `datasets` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Installing necessary packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-06T11:32:40.966287Z",
     "iopub.status.busy": "2025-07-06T11:32:40.965974Z",
     "iopub.status.idle": "2025-07-06T11:34:05.888928Z",
     "shell.execute_reply": "2025-07-06T11:34:05.888048Z",
     "shell.execute_reply.started": "2025-07-06T11:32:40.966237Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --quiet\n",
    "!pip install datasets sacrebleu --quiet\n",
    "!pip install tokenizers --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Importing libraries and loading the dataset from hugging face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T11:34:05.890586Z",
     "iopub.status.busy": "2025-07-06T11:34:05.890307Z",
     "iopub.status.idle": "2025-07-06T11:37:44.626427Z",
     "shell.execute_reply": "2025-07-06T11:37:44.625686Z",
     "shell.execute_reply.started": "2025-07-06T11:34:05.890562Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44856b1cc804effb7c2c19d55a02414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186f4086f629465c8b0dc0e2dc4269f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "iwslt2017.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repository for iwslt2017 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/iwslt2017.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9219ae9551e478e8c2d61c323daa46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "en-de.zip:   0%|          | 0.00/16.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08023c446cc48ccb8f46365f5079994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/206112 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b4ed95672d4502ba86cff7e98e9517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/8079 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b878f5da8b4858af8adabf93e31da2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/888 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'de': 'Vielen Dank, Chris.', 'en': 'Thank you so much, Chris.'}}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "import sacrebleu\n",
    "import time\n",
    "from tokenizers import ByteLevelBPETokenizer \n",
    "\n",
    "raw_datasets = load_dataset(\"iwslt2017\", \"iwslt2017-en-de\")\n",
    "train_data = raw_datasets['train']\n",
    "val_data = raw_datasets['validation']\n",
    "test_data = raw_datasets['test']\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. BPE Tokenizer Training and Vocabulary Creation\n",
    "\n",
    "- Prepares text files for English and German translations from the training data.\n",
    "- Trains Byte Pair Encoding (BPE) tokenizers for English and German with a vocabulary size of 10,000.\n",
    "- Defines special tokens (`<pad>`, `<sos>`, `<eos>`, `<unk>`) for handling padding, start/end of sequences, and unknown tokens.\n",
    "- Extracts vocabularies and token-to-ID mappings for both languages.\n",
    "- Prints vocabulary sizes and IDs of special tokens for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T11:37:44.627871Z",
     "iopub.status.busy": "2025-07-06T11:37:44.627546Z",
     "iopub.status.idle": "2025-07-06T11:38:05.904709Z",
     "shell.execute_reply": "2025-07-06T11:38:05.903967Z",
     "shell.execute_reply.started": "2025-07-06T11:37:44.627852Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "EN vocab size: 10000, DE vocab size: 10000\n",
      "EN <pad> ID: 0, DE <pad> ID: 0\n"
     ]
    }
   ],
   "source": [
    "with open(\"en_texts.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for text in [x['translation']['en'] for x in train_data]:\n",
    "        f.write(text + \"\\n\")\n",
    "\n",
    "with open(\"de_texts.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for text in [x['translation']['de'] for x in train_data]:\n",
    "        f.write(text + \"\\n\")\n",
    "\n",
    "en_tokenizer = ByteLevelBPETokenizer()\n",
    "en_tokenizer.train(files=[\"en_texts.txt\"], vocab_size=10000, min_frequency=2, special_tokens=[\n",
    "    \"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"\n",
    "])\n",
    "\n",
    "de_tokenizer = ByteLevelBPETokenizer()\n",
    "de_tokenizer.train(files=[\"de_texts.txt\"], vocab_size=10000, min_frequency=2, special_tokens=[\n",
    "    \"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"\n",
    "])\n",
    "\n",
    "SRC_vocab = en_tokenizer.get_vocab()\n",
    "TGT_vocab = de_tokenizer.get_vocab()\n",
    "\n",
    "SRC_itos = {i: en_tokenizer.id_to_token(i) for i in range(len(SRC_vocab))}\n",
    "TGT_itos = {i: de_tokenizer.id_to_token(i) for i in range(len(TGT_vocab))}\n",
    "\n",
    "PAD_IDX_EN = en_tokenizer.token_to_id(\"<pad>\")\n",
    "SOS_IDX_EN = en_tokenizer.token_to_id(\"<sos>\")\n",
    "EOS_IDX_EN = en_tokenizer.token_to_id(\"<eos>\")\n",
    "UNK_IDX_EN = en_tokenizer.token_to_id(\"<unk>\")\n",
    "\n",
    "PAD_IDX_DE = de_tokenizer.token_to_id(\"<pad>\")\n",
    "SOS_IDX_DE = de_tokenizer.token_to_id(\"<sos>\")\n",
    "EOS_IDX_DE = de_tokenizer.token_to_id(\"<eos>\")\n",
    "UNK_IDX_DE = de_tokenizer.token_to_id(\"<unk>\")\n",
    "\n",
    "print(f\"EN vocab size: {len(SRC_vocab)}, DE vocab size: {len(TGT_vocab)}\")\n",
    "print(f\"EN <pad> ID: {PAD_IDX_EN}, DE <pad> ID: {PAD_IDX_DE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Dataset and DataLoader (updated for BPE)\n",
    "- Defines a function `encode_bpe` to encode text using BPE tokenizers and add `<sos>` and `<eos>` tokens.\n",
    "- Implements `TranslationDataset` to process source and target sequences using BPE tokenization.\n",
    "- Defines `collate_fn` for padding sequences in batches.\n",
    "- Creates `DataLoader` objects for training, validation, and test datasets with a batch size of 64.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T11:38:05.906664Z",
     "iopub.status.busy": "2025-07-06T11:38:05.906443Z",
     "iopub.status.idle": "2025-07-06T11:38:53.901805Z",
     "shell.execute_reply": "2025-07-06T11:38:53.900985Z",
     "shell.execute_reply.started": "2025-07-06T11:38:05.906648Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "MAX_LEN = 64 \n",
    "\n",
    "def encode_bpe(text, tokenizer, max_len=MAX_LEN):\n",
    "    encoded = tokenizer.encode(text)\n",
    "    \n",
    "    ids = encoded.ids\n",
    "   \n",
    "    res_ids = [tokenizer.token_to_id(\"<sos>\")] + ids + [tokenizer.token_to_id(\"<eos>\")]\n",
    "    \n",
    "    if len(res_ids) > max_len:\n",
    "        res_ids = res_ids[:max_len-1] + [tokenizer.token_to_id(\"<eos>\")]\n",
    "    return res_ids\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data, en_tokenizer, de_tokenizer):\n",
    "        self.src = [encode_bpe(x['translation']['en'], en_tokenizer) for x in data]\n",
    "        self.tgt = [encode_bpe(x['translation']['de'], de_tokenizer) for x in data]\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.src[idx], dtype=torch.long), torch.tensor(self.tgt[idx], dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src, tgt = zip(*batch)\n",
    "    src = nn.utils.rnn.pad_sequence(src, padding_value=PAD_IDX_EN)\n",
    "    tgt = nn.utils.rnn.pad_sequence(tgt, padding_value=PAD_IDX_DE)\n",
    "    return src, tgt\n",
    "\n",
    "train_ds = TranslationDataset(train_data, en_tokenizer, de_tokenizer)\n",
    "val_ds   = TranslationDataset(val_data, en_tokenizer, de_tokenizer)\n",
    "test_ds  = TranslationDataset(test_data, en_tokenizer, de_tokenizer)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Transformer Model Definition \n",
    "- Defines a Transformer-based sequence-to-sequence model for translation.\n",
    "- Includes:\n",
    "  - Token embeddings for source and target languages with padding indices.\n",
    "  - Positional embeddings for sequence positions.\n",
    "  - Transformer architecture with encoder and decoder layers.\n",
    "  - A linear layer (`generator`) to predict target tokens.\n",
    "- Handles padding masks for source and target sequences during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T11:38:53.902798Z",
     "iopub.status.busy": "2025-07-06T11:38:53.902610Z",
     "iopub.status.idle": "2025-07-06T11:38:53.910342Z",
     "shell.execute_reply": "2025-07-06T11:38:53.909674Z",
     "shell.execute_reply.started": "2025-07-06T11:38:53.902784Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, emb_dim=256, nhead=4, num_layers=3, dim_feedforward=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.src_tok_emb = nn.Embedding(src_vocab_size, emb_dim, padding_idx=PAD_IDX_EN)\n",
    "        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, emb_dim, padding_idx=PAD_IDX_DE)\n",
    "        \n",
    "        self.pos_encoder = nn.Embedding(MAX_LEN + 2, emb_dim)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=emb_dim, nhead=nhead, num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers, dim_feedforward=dim_feedforward, dropout=dropout,\n",
    "            batch_first=False\n",
    "        )\n",
    "        self.generator = nn.Linear(emb_dim, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_padding_mask=None, tgt_padding_mask=None):\n",
    "        src_seq_len, N = src.shape\n",
    "        tgt_seq_len, N = tgt.shape\n",
    "\n",
    "        src_pos = torch.arange(0, src_seq_len, device=src.device).unsqueeze(1).expand(src_seq_len, N)\n",
    "        tgt_pos = torch.arange(0, tgt_seq_len, device=tgt.device).unsqueeze(1).expand(tgt_seq_len, N)\n",
    "\n",
    "        src_emb = self.src_tok_emb(src) + self.pos_encoder(src_pos)\n",
    "        tgt_emb = self.tgt_tok_emb(tgt) + self.pos_encoder(tgt_pos)\n",
    "\n",
    "        src_padding_mask = (src == PAD_IDX_EN).transpose(0, 1)\n",
    "        tgt_padding_mask = (tgt == PAD_IDX_DE).transpose(0, 1)\n",
    "\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_seq_len).to(src.device)\n",
    "\n",
    "        out = self.transformer(\n",
    "            src_emb, tgt_emb,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_padding_mask,\n",
    "            memory_key_padding_mask=src_padding_mask\n",
    "        )\n",
    "        return self.generator(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Training and Evaluation Functions \n",
    "- Configures the model, optimizer, and loss function (`CrossEntropyLoss` with padding ignored).\n",
    "- Implements:\n",
    "  - `train_epoch`: Trains the model for one epoch using teacher forcing.\n",
    "  - `evaluate`: Evaluates the model on validation data.\n",
    "  - `translate`: Translates a source sentence using greedy decoding.\n",
    "  - `calc_bleu`: Computes BLEU score for translation quality using `sacrebleu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T11:38:53.911217Z",
     "iopub.status.busy": "2025-07-06T11:38:53.910991Z",
     "iopub.status.idle": "2025-07-06T11:38:58.928457Z",
     "shell.execute_reply": "2025-07-06T11:38:58.927681Z",
     "shell.execute_reply.started": "2025-07-06T11:38:53.911194Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = TransformerModel(len(SRC_vocab), len(TGT_vocab)).to(DEVICE)\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX_DE)\n",
    "\n",
    "def train_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in loader:\n",
    "        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(src, tgt[:-1, :])\n",
    "        out = out.reshape(-1, out.shape[-1])\n",
    "        tgt_y = tgt[1:, :].reshape(-1)\n",
    "        loss = criterion(out, tgt_y)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in loader:\n",
    "            src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
    "            out = model(src, tgt[:-1, :])\n",
    "            out = out.reshape(-1, out.shape[-1])\n",
    "            tgt_y = tgt[1:, :].reshape(-1)\n",
    "            loss = criterion(out, tgt_y)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def translate(model, src_sentence, max_len=MAX_LEN):\n",
    "    model.eval()\n",
    "    src_ids = torch.tensor(encode_bpe(src_sentence, en_tokenizer), dtype=torch.long).unsqueeze(1).to(DEVICE)\n",
    "    \n",
    "    src_len = src_ids.shape[0]\n",
    "    src_pos = torch.arange(0, src_len, device=DEVICE).unsqueeze(1)\n",
    "    src_emb = model.src_tok_emb(src_ids) + model.pos_encoder(src_pos)\n",
    "    \n",
    "    src_padding_mask_single = (src_ids == PAD_IDX_EN).transpose(0, 1)\n",
    "    memory = model.transformer.encoder(src_emb, src_key_padding_mask=src_padding_mask_single)\n",
    "    \n",
    "    ys = torch.tensor([[SOS_IDX_DE]], dtype=torch.long).to(DEVICE)\n",
    "    \n",
    "    for i in range(max_len - 1):\n",
    "        tgt_pos = torch.arange(0, ys.shape[0], device=DEVICE).unsqueeze(1)\n",
    "        tgt_emb = model.tgt_tok_emb(ys) + model.pos_encoder(tgt_pos)\n",
    "        \n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(ys.shape[0]).to(DEVICE)\n",
    "        \n",
    "        out = model.transformer.decoder(\n",
    "            tgt_emb,\n",
    "            memory,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_key_padding_mask=src_padding_mask_single\n",
    "        )\n",
    "        \n",
    "        logits = model.generator(out[-1, :])\n",
    "        next_word_id = logits.argmax(1).item()\n",
    "        \n",
    "        ys = torch.cat([ys, torch.tensor([[next_word_id]], device=DEVICE)], dim=0)\n",
    "        \n",
    "        if next_word_id == EOS_IDX_DE:\n",
    "            break\n",
    "    \n",
    "    pred_tokens_ids = [id_val for id_val in ys[1:-1, 0].cpu().numpy() if id_val not in [PAD_IDX_DE, SOS_IDX_DE, EOS_IDX_DE]]\n",
    "    \n",
    "    pred_sentence = de_tokenizer.decode(pred_tokens_ids)\n",
    "    return pred_sentence\n",
    "\n",
    "def calc_bleu(model, loader, num_batches=30):\n",
    "    refs, hyps = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (src_batch, tgt_batch) in enumerate(loader):\n",
    "            if i >= num_batches: break\n",
    "            for b in range(src_batch.shape[1]):\n",
    "                src_ids_original = [id.item() for id in src_batch[:, b].cpu().numpy() if id != PAD_IDX_EN]\n",
    "                src_decoded_text = en_tokenizer.decode([id_val for id_val in src_ids_original if id_val not in [SOS_IDX_EN, EOS_IDX_EN, PAD_IDX_EN]])\n",
    "\n",
    "                tgt_ids_original = [id.item() for id in tgt_batch[:, b].cpu().numpy() if id != PAD_IDX_DE]\n",
    "                tgt_decoded_text = de_tokenizer.decode([id_val for id_val in tgt_ids_original if id_val not in [SOS_IDX_DE, EOS_IDX_DE, PAD_IDX_DE]])\n",
    "                \n",
    "                pred = translate(model, src_decoded_text)\n",
    "                \n",
    "                refs.append([tgt_decoded_text])\n",
    "                hyps.append(pred)\n",
    "                \n",
    "    bleu = sacrebleu.corpus_bleu(hyps, list(zip(*refs)))\n",
    "    return bleu.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Training Loop and Final Evaluation \n",
    "- Running the training loop for 20 epochs, printing training loss, validation loss, and BLEU score after each epoch.\n",
    "- Saves the model with the best validation BLEU score.\n",
    "- Loads the best model for final evaluation on the test set.\n",
    "- Computes BLEU score on the test set and prints example translations for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T11:38:58.930206Z",
     "iopub.status.busy": "2025-07-06T11:38:58.929327Z",
     "iopub.status.idle": "2025-07-06T13:01:08.287641Z",
     "shell.execute_reply": "2025-07-06T13:01:08.286971Z",
     "shell.execute_reply.started": "2025-07-06T11:38:58.930177Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop...\n",
      "\n",
      "--- Starting Epoch 1/20 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 5.559 | Val Loss: 4.881 | Val BLEU: 5.60 | Time: 244.4s\n",
      "*** New best validation BLEU: 5.60. Model saved to 'best_transformer_model_bleu.pt'! ***\n",
      "\n",
      "--- Starting Epoch 2/20 ---\n",
      "Epoch 2 | Train Loss: 4.486 | Val Loss: 4.077 | Val BLEU: 9.01 | Time: 248.0s\n",
      "*** New best validation BLEU: 9.01. Model saved to 'best_transformer_model_bleu.pt'! ***\n",
      "\n",
      "--- Starting Epoch 3/20 ---\n",
      "Epoch 3 | Train Loss: 3.868 | Val Loss: 3.574 | Val BLEU: 11.70 | Time: 245.9s\n",
      "*** New best validation BLEU: 11.70. Model saved to 'best_transformer_model_bleu.pt'! ***\n",
      "\n",
      "--- Starting Epoch 4/20 ---\n",
      "Epoch 4 | Train Loss: 3.478 | Val Loss: 3.309 | Val BLEU: 13.58 | Time: 247.5s\n",
      "*** New best validation BLEU: 13.58. Model saved to 'best_transformer_model_bleu.pt'! ***\n",
      "\n",
      "--- Starting Epoch 5/20 ---\n",
      "Epoch 5 | Train Loss: 3.224 | Val Loss: 3.113 | Val BLEU: 15.50 | Time: 246.3s\n",
      "*** New best validation BLEU: 15.50. Model saved to 'best_transformer_model_bleu.pt'! ***\n",
      "\n",
      "--- Starting Epoch 6/20 ---\n",
      "Epoch 6 | Train Loss: 3.042 | Val Loss: 2.988 | Val BLEU: 15.81 | Time: 247.5s\n",
      "*** New best validation BLEU: 15.81. Model saved to 'best_transformer_model_bleu.pt'! ***\n",
      "\n",
      "--- Starting Epoch 7/20 ---\n",
      "Epoch 7 | Train Loss: 2.902 | Val Loss: 2.912 | Val BLEU: 16.34 | Time: 246.4s\n",
      "*** New best validation BLEU: 16.34. Model saved to 'best_transformer_model_bleu.pt'! ***\n",
      "\n",
      "--- Starting Epoch 8/20 ---\n",
      "Epoch 8 | Train Loss: 2.788 | Val Loss: 2.815 | Val BLEU: 17.52 | Time: 246.6s\n",
      "*** New best validation BLEU: 17.52. Model saved to 'best_transformer_model_bleu.pt'! ***\n",
      "\n",
      "--- Starting Epoch 9/20 ---\n",
      "Epoch 9 | Train Loss: 2.694 | Val Loss: 2.749 | Val BLEU: 17.79 | Time: 245.8s\n",
      "*** New best validation BLEU: 17.79. Model saved to 'best_transformer_model_bleu.pt'! ***\n",
      "\n",
      "--- Starting Epoch 10/20 ---\n",
      "Epoch 10 | Train Loss: 2.613 | Val Loss: 2.705 | Val BLEU: 18.29 | Time: 246.2s\n",
      "*** New best validation BLEU: 18.29. Model saved to 'best_transformer_model_bleu.pt'! ***\n",
      "\n",
      "--- Starting Epoch 11/20 ---\n",
      "Epoch 11 | Train Loss: 2.543 | Val Loss: 2.644 | Val BLEU: 18.59 | Time: 246.8s\n",
      "*** New best validation BLEU: 18.59. Model saved to 'best_transformer_model_bleu.pt'! ***\n",
      "\n",
      "--- Starting Epoch 12/20 ---\n",
      "Epoch 12 | Train Loss: 2.482 | Val Loss: 2.609 | Val BLEU: 18.70 | Time: 246.7s\n",
      "*** New best validation BLEU: 18.70. Model saved to 'best_transformer_model_bleu.pt'! ***\n",
      "\n",
      "--- Starting Epoch 13/20 ---\n",
      "Epoch 13 | Train Loss: 2.429 | Val Loss: 2.582 | Val BLEU: 19.29 | Time: 245.5s\n",
      "*** New best validation BLEU: 19.29. Model saved to 'best_transformer_model_bleu.pt'! ***\n",
      "\n",
      "--- Starting Epoch 14/20 ---\n",
      "Epoch 14 | Train Loss: 2.381 | Val Loss: 2.564 | Val BLEU: 19.58 | Time: 246.1s\n",
      "*** New best validation BLEU: 19.58. Model saved to 'best_transformer_model_bleu.pt'! ***\n",
      "\n",
      "--- Starting Epoch 15/20 ---\n",
      "Epoch 15 | Train Loss: 2.338 | Val Loss: 2.524 | Val BLEU: 20.03 | Time: 246.7s\n",
      "*** New best validation BLEU: 20.03. Model saved to 'best_transformer_model_bleu.pt'! ***\n",
      "\n",
      "--- Starting Epoch 16/20 ---\n",
      "Epoch 16 | Train Loss: 2.298 | Val Loss: 2.512 | Val BLEU: 19.93 | Time: 246.1s\n",
      "Validation BLEU did not improve. Current best: 20.03\n",
      "\n",
      "--- Starting Epoch 17/20 ---\n",
      "Epoch 17 | Train Loss: 2.263 | Val Loss: 2.503 | Val BLEU: 20.03 | Time: 247.7s\n",
      "Validation BLEU did not improve. Current best: 20.03\n",
      "\n",
      "--- Starting Epoch 18/20 ---\n",
      "Epoch 18 | Train Loss: 2.230 | Val Loss: 2.495 | Val BLEU: 20.79 | Time: 245.9s\n",
      "*** New best validation BLEU: 20.79. Model saved to 'best_transformer_model_bleu.pt'! ***\n",
      "\n",
      "--- Starting Epoch 19/20 ---\n",
      "Epoch 19 | Train Loss: 2.200 | Val Loss: 2.456 | Val BLEU: 20.75 | Time: 245.8s\n",
      "Validation BLEU did not improve. Current best: 20.79\n",
      "\n",
      "--- Starting Epoch 20/20 ---\n",
      "Epoch 20 | Train Loss: 2.172 | Val Loss: 2.445 | Val BLEU: 20.31 | Time: 245.9s\n",
      "Validation BLEU did not improve. Current best: 20.79\n",
      "\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch.nn as nn\n",
    "\n",
    "EPOCHS = 20 \n",
    "\n",
    "best_val_bleu = -1.0\n",
    "model_save_path = 'best_transformer_model_bleu.pt' \n",
    "\n",
    "print(\"Starting training loop...\")\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    start = time.time()\n",
    "    print(f\"\\n--- Starting Epoch {epoch}/{EPOCHS} ---\")\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader)\n",
    "    val_loss = evaluate(model, val_loader)\n",
    "    \n",
    "\n",
    "    val_bleu = calc_bleu(model, val_loader, num_batches=30) \n",
    "    \n",
    "    print(f\"Epoch {epoch} | Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f} | Val BLEU: {val_bleu:.2f} | Time: {time.time()-start:.1f}s\")\n",
    "    \n",
    "    if val_bleu > best_val_bleu:\n",
    "        best_val_bleu = val_bleu\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"New best validation BLEU: {best_val_bleu:.2f}. Model saved to '{model_save_path}'! ***\")\n",
    "    else:\n",
    "        print(f\"Validation BLEU did not improve. Current best: {best_val_bleu:.2f}\")\n",
    "\n",
    "\n",
    "print(\"\\nTraining complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights:\n",
    "\n",
    "\n",
    "Model Performance: \n",
    "- The Transformer model achieved a final validation BLEU score of 20.79, indicating good translation quality for the English-German dataset.\n",
    "- BLEU scores steadily improved across epochs, demonstrating effective learning and optimization.\n",
    "\n",
    "BLEU score:\n",
    "- BLEU scores above 20 are typical for small-scale experiments like this one.\n",
    "- Real-world datasets with larger and more diverse data aim for BLEU scores in the range of 30-40+.\n",
    "\n",
    "The model demonstrates reasonable translation quality for the IWSLT2017 dataset, which is suitable for benchmarking translation models on small-scale datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Final Test BLEU Calculation and Example Translations\n",
    "- Loads the best model saved during training.\n",
    "- Computes BLEU score on the test set using all batches.\n",
    "- Prints example translations for comparison between source, target, and predicted sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-06T13:01:08.288648Z",
     "iopub.status.busy": "2025-07-06T13:01:08.288432Z",
     "iopub.status.idle": "2025-07-06T13:11:43.033400Z",
     "shell.execute_reply": "2025-07-06T13:11:43.032617Z",
     "shell.execute_reply.started": "2025-07-06T13:01:08.288629Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model for final evaluation on test set...\n",
      "\n",
      "--- Final Test BLEU Calculation ---\n",
      "Test BLEU: 22.07\n",
      "\n",
      "--- Example Translations ---\n",
      "EN: Several years ago here at TED, Peter Skillman  introduced a design challenge  called the marshmallow challenge.\n",
      "DE: Vor einigen Jahren, hier bei TED, stellte Peter Skillman einen Design-Wettbewerb namens \"Die Marshmallow-Herausforderung\" vor.\n",
      "PRED: Vor einigen Jahren hier bei TED, Peter Skillman  stellte eine Design-Herausforderung  namens \"Maymeistungs-Herausforderung\".\n",
      "---\n",
      "EN: And the idea's pretty simple:  Teams of four have to build the tallest free-standing structure  out of 20 sticks of spaghetti,  one yard of tape, one yard of string  and a marshmallow.\n",
      "DE: Die Idee ist ziemlich einfach. Vierer-teams müssen die größtmögliche freistehende Struktur mit 20 Spaghetti, ca. 1m Klebeband, ca. 1m Faden und einem Marshmallow bauen.\n",
      "PRED: Und die Idee ist ziemlich einfach:  Teleams von vier müssen die höchsten kostenlose Struktur bauen,  aus 20 Stöcken von Spaghetti,  einer Yardoard of Klebeband, ein Yardo und ein Murmmaphall.\n",
      "---\n",
      "EN: The marshmallow has to be on top.\n",
      "DE: Der Marshmallow muss oben drauf sein.\n",
      "PRED: Der Marshmallowatt muss oben sein.\n",
      "---\n",
      "EN: And, though it seems really simple, it's actually pretty hard  because it forces people  to collaborate very quickly.\n",
      "DE: Und, obwohl es wirklich einfach scheint, ist es tatsächlich richtig schwer, weil es Leute drängt sehr schnell zusammenzuarbeiten.\n",
      "PRED: Und obwohl es wirklich einfach erscheint, ist es wirklich ziemlich hart  denn es ist sehr schwierig, die Leute  zu kooperieren.\n",
      "---\n",
      "EN: And so, I thought this was an interesting idea,  and I incorporated it into a design workshop.\n",
      "DE: Und so dachte ich, dass dies eine interessante Idee ist und ich habe es in einen Design-Workshop verwandelt.\n",
      "PRED: Und so dachte ich, das war eine interessante Idee,  und ich habe es in einen Design-Hopam gefasst.\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading best model for final evaluation on test set...\")\n",
    "model.load_state_dict(torch.load(model_save_path, map_location=DEVICE))\n",
    "\n",
    "print(\"\\n--- Final Test BLEU Calculation ---\")\n",
    "\n",
    "test_bleu = calc_bleu(model, test_loader, num_batches=len(test_loader))\n",
    "print(f\"Test BLEU: {test_bleu:.2f}\")\n",
    "\n",
    "print(\"\\n--- Example Translations ---\")\n",
    "for i in range(5): \n",
    "    en_orig = test_data[i]['translation']['en']\n",
    "    de_orig = test_data[i]['translation']['de']\n",
    "    pred_de = translate(model, en_orig)\n",
    "    print(f\"EN: {en_orig}\\nDE: {de_orig}\\nPRED: {pred_de}\\n---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "- The model achieved a Test BLEU score of 22.07, indicating good translation quality for the English-German dataset.\n",
    "-  Translations are generally accurate but contain minor errors in rare words and complex sentences.\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
