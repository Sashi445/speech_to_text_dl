{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Text translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Text Translation Setup**\n",
        "\n",
        "- Imports required libraries for sequence-to-sequence text translation using PyTorch.\n",
        "- Configures device to use GPU if available, otherwise CPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dataset Description\n",
        "\n",
        "We use the **English-French sentence pair dataset** from [manythings.org](http://www.manythings.org/anki/fra-eng.zip).\n",
        "\n",
        "- Contains parallel English and French sentences for basic machine translation tasks.\n",
        "- Designed for beginners to explore translation models without large compute requirements.\n",
        "- Suitable for quick experimentation and understanding of sequence-to-sequence models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SoOPs_odcoAb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dataset Preparation for Text Translation\n",
        "\n",
        "- Downloads and extracts the English-French sentence pair dataset.\n",
        "- Defines `normalize` function to lowercase, clean, and simplify text.\n",
        "- Loads sentence pairs from `fra.txt` and applies normalization.\n",
        "- Keeps the first 10,000 sentence pairs for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyzaI207cuFT",
        "outputId": "5aa410e3-8ca5-41ae-a0d1-698911f8463d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-06-23 06:29:20--  http://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8143096 (7.8M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip.2’\n",
            "\n",
            "\rfra-eng.zip.2         0%[                    ]       0  --.-KB/s               \rfra-eng.zip.2       100%[===================>]   7.77M  41.4MB/s    in 0.2s    \n",
            "\n",
            "2025-06-23 06:29:21 (41.4 MB/s) - ‘fra-eng.zip.2’ saved [8143096/8143096]\n",
            "\n",
            "caution: filename not matched:  -y\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import unicodedata\n",
        "import re\n",
        "\n",
        "!wget http://www.manythings.org/anki/fra-eng.zip\n",
        "!unzip -q fra-eng.zip -y\n",
        "\n",
        "def normalize(s):\n",
        "    s = unicodedata.normalize('NFD', s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s.strip()\n",
        "\n",
        "with open(\"fra.txt\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().strip().split('\\n')\n",
        "\n",
        "pairs = [[normalize(s) for s in l.split('\\t')[:2]] for l in lines if len(l.split('\\t')) >= 2]\n",
        "pairs = pairs[:10000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Vocabulary Creation\n",
        "\n",
        "- `tokenize`: Splits sentences into word tokens.\n",
        "- `build_vocab`: Builds word-to-index vocabulary for source and target languages with special tokens (`<pad>`, `<sos>`, `<eos>`, `<unk>`).\n",
        "- Creates source (`SRC_vocab`) and target (`TGT_vocab`) vocabularies along with index-to-word mappings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "M2acXKGzcx-K"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def tokenize(s): return s.split()\n",
        "\n",
        "def build_vocab(pairs, idx):\n",
        "    counter = Counter()\n",
        "    for pair in pairs:\n",
        "        counter.update(tokenize(pair[idx]))\n",
        "    vocab = {'<pad>':0, '<sos>':1, '<eos>':2, '<unk>':3}\n",
        "    for word in counter:\n",
        "        vocab[word] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "SRC_vocab = build_vocab(pairs, 0)\n",
        "TGT_vocab = build_vocab(pairs, 1)\n",
        "\n",
        "SRC_itos = {i:s for s,i in SRC_vocab.items()}\n",
        "TGT_itos = {i:s for s,i in TGT_vocab.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Data Processing and Batching\n",
        "\n",
        "- `encode`: Converts a sentence to a list of token indices, using `<unk>` for unknown words.\n",
        "- `tensorify`: Converts sentence pairs to source and target tensors with `<sos>` and `<eos>` tokens.\n",
        "- Splits data into training and validation sets (90/10 split).\n",
        "- `collate_fn`: Pads source and target sequences for batch processing.\n",
        "- Creates DataLoaders for training and validation with batch size 32.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "GkuZoMhqc11V"
      },
      "outputs": [],
      "source": [
        "def encode(sentence, vocab):\n",
        "    return [vocab.get(word, vocab['<unk>']) for word in tokenize(sentence)]\n",
        "\n",
        "def tensorify(pair):\n",
        "    src = torch.tensor([SRC_vocab['<sos>']] + encode(pair[0], SRC_vocab) + [SRC_vocab['<eos>']], dtype=torch.long)\n",
        "    tgt = torch.tensor([TGT_vocab['<sos>']] + encode(pair[1], TGT_vocab) + [TGT_vocab['<eos>']], dtype=torch.long)\n",
        "    return src, tgt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_pairs, val_pairs = train_test_split(pairs, test_size=0.1, random_state=42)\n",
        "tensor_train = [tensorify(p) for p in train_pairs]\n",
        "tensor_val   = [tensorify(p) for p in val_pairs]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "    src_pad = pad_sequence(src_batch, padding_value=SRC_vocab['<pad>'])\n",
        "    tgt_pad = pad_sequence(tgt_batch, padding_value=TGT_vocab['<pad>'])\n",
        "    return src_pad, tgt_pad\n",
        "\n",
        "loader = DataLoader(tensor_train, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader   = DataLoader(tensor_val, batch_size=32, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Encoder-Decoder Model\n",
        "\n",
        "- `Encoder`:  \n",
        "  - Embeds source tokens and processes them with a GRU.  \n",
        "  - Returns output features and final hidden state.  \n",
        "\n",
        "- `Decoder`:  \n",
        "  - Embeds target tokens and processes them with a GRU.  \n",
        "  - Uses a linear layer to predict the next token.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "SV4VFVCSc4qy"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src)\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        return outputs, hidden\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embed(input.unsqueeze(0))\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        return prediction, hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model Training and Evaluation\n",
        "\n",
        "- Defines embedding and hidden dimensions for encoder and decoder.\n",
        "- Initializes encoder, decoder, optimizers, and loss function (`CrossEntropyLoss` with padding ignored).\n",
        "- `train_one_epoch`: Trains the model using teacher forcing and computes average training loss.\n",
        "- `evaluate_val_loss`: Evaluates model on validation set without gradient updates.\n",
        "- Runs training for 15 epochs, printing training and validation loss after each epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SPCnoIqc7MD",
        "outputId": "fde92048-24ed-4ea3-bb2b-55d8235b9f25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 2.9507, Val Loss: 2.2762\n",
            "Epoch 2, Train Loss: 1.9661, Val Loss: 1.9119\n",
            "Epoch 3, Train Loss: 1.5639, Val Loss: 1.7394\n",
            "Epoch 4, Train Loss: 1.2855, Val Loss: 1.6296\n",
            "Epoch 5, Train Loss: 1.0933, Val Loss: 1.5382\n",
            "Epoch 6, Train Loss: 0.9226, Val Loss: 1.4748\n",
            "Epoch 7, Train Loss: 0.7918, Val Loss: 1.4445\n",
            "Epoch 8, Train Loss: 0.6784, Val Loss: 1.4161\n",
            "Epoch 9, Train Loss: 0.5888, Val Loss: 1.3974\n",
            "Epoch 10, Train Loss: 0.5145, Val Loss: 1.3863\n",
            "Epoch 11, Train Loss: 0.4458, Val Loss: 1.3980\n",
            "Epoch 12, Train Loss: 0.3907, Val Loss: 1.3776\n",
            "Epoch 13, Train Loss: 0.3432, Val Loss: 1.3904\n",
            "Epoch 14, Train Loss: 0.3075, Val Loss: 1.3953\n",
            "Epoch 15, Train Loss: 0.2804, Val Loss: 1.4015\n"
          ]
        }
      ],
      "source": [
        "INPUT_DIM = len(SRC_vocab)\n",
        "OUTPUT_DIM = len(TGT_vocab)\n",
        "HID_DIM = 256\n",
        "EMB_DIM = 128\n",
        "\n",
        "encoder = Encoder(INPUT_DIM, EMB_DIM, HID_DIM).to(device)\n",
        "decoder = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM).to(device)\n",
        "\n",
        "enc_opt = optim.Adam(encoder.parameters())\n",
        "dec_opt = optim.Adam(decoder.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TGT_vocab['<pad>'])\n",
        "\n",
        "def train_one_epoch():\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    total_loss = 0\n",
        "    for src, tgt in loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        enc_opt.zero_grad(); dec_opt.zero_grad()\n",
        "\n",
        "        hidden = encoder(src)\n",
        "        input_dec = tgt[0,:]\n",
        "        loss = 0\n",
        "\n",
        "        for t in range(1, tgt.shape[0]):\n",
        "            pred, hidden = decoder(input_dec, hidden)\n",
        "            loss += criterion(pred, tgt[t])\n",
        "            input_dec = tgt[t]  # teacher forcing\n",
        "\n",
        "        loss.backward()\n",
        "        enc_opt.step(); dec_opt.step()\n",
        "        total_loss += loss.item() / (tgt.shape[0] - 1)\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def evaluate_val_loss():\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in val_loader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            hidden = encoder(src)\n",
        "\n",
        "            input_dec = tgt[0, :]\n",
        "            loss = 0\n",
        "\n",
        "            for t in range(1, tgt.shape[0]):\n",
        "                pred, hidden = decoder(input_dec, hidden)\n",
        "                loss += criterion(pred, tgt[t])\n",
        "                input_dec = tgt[t]  # teacher forcing\n",
        "\n",
        "            total_loss += loss.item() / (tgt.shape[0] - 1)\n",
        "\n",
        "    return total_loss / len(val_loader)\n",
        "\n",
        "for epoch in range(1, 16):\n",
        "    train_loss = train_one_epoch()\n",
        "    val_loss = evaluate_val_loss()\n",
        "    print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Translation Function\n",
        "\n",
        "- `translate`: Translates a given source sentence using the trained encoder-decoder model.\n",
        "- Performs greedy decoding until `<eos>` token or maximum length is reached.\n",
        "- Returns the generated target sentence as text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "rviQzOMHc9Tk"
      },
      "outputs": [],
      "source": [
        "def translate(sentence, max_len=20):\n",
        "    encoder.eval(); decoder.eval()\n",
        "    with torch.no_grad():\n",
        "        src_tensor = torch.tensor([SRC_vocab['<sos>']] + encode(sentence, SRC_vocab) + [SRC_vocab['<eos>']], device=device).unsqueeze(1)\n",
        "        hidden = encoder(src_tensor)\n",
        "\n",
        "        input_dec = torch.tensor([TGT_vocab['<sos>']], device=device)\n",
        "        output_sentence = []\n",
        "\n",
        "        for _ in range(max_len):\n",
        "            pred, hidden = decoder(input_dec, hidden)\n",
        "            top1 = pred.argmax(1).item()\n",
        "            if top1 == TGT_vocab['<eos>']:\n",
        "                break\n",
        "            output_sentence.append(TGT_itos[top1])\n",
        "            input_dec = torch.tensor([top1], device=device)\n",
        "\n",
        "    return ' '.join(output_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### BLEU Score Evaluation Setup\n",
        "\n",
        "- Installs `sacrebleu` library for evaluating translation quality using BLEU scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzMuhjfQdtby",
        "outputId": "58512e8d-0000-4026-9a77-671f9e789c51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2.0.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-3.2.0 sacrebleu-2.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sacrebleu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### BLEU Score Evaluation\n",
        "\n",
        "- `evaluate_bleu_sacre`: Computes BLEU score using `sacrebleu` on random validation samples.\n",
        "- Compares model translations to reference sentences to assess translation quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "exjgSMSDdoyJ"
      },
      "outputs": [],
      "source": [
        "import sacrebleu\n",
        "\n",
        "def evaluate_bleu_sacre(translate_fn, val_pairs, n_samples=100):\n",
        "    refs = []\n",
        "    hyps = []\n",
        "\n",
        "    samples = random.sample(val_pairs, n_samples)\n",
        "\n",
        "    for src_tensor, tgt_tensor in samples:\n",
        "        src_sentence = ' '.join([SRC_itos[i.item()] for i in src_tensor[1:-1]])\n",
        "        tgt_sentence = ' '.join([TGT_itos[i.item()] for i in tgt_tensor[1:-1]])\n",
        "        pred = translate_fn(src_sentence)\n",
        "\n",
        "        refs.append([tgt_sentence])\n",
        "        hyps.append(pred)\n",
        "\n",
        "    bleu = sacrebleu.corpus_bleu(hyps, list(zip(*refs)))\n",
        "    print(f\"BLEU score (sacreBLEU): {bleu.score:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run BLEU Score Evaluation\n",
        "\n",
        "- Evaluates translation quality on 100 random validation samples using `sacrebleu`.\n",
        "- Prints the overall BLEU score for the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjzEzIeLd8WR",
        "outputId": "1ceef39a-31d0-4bdd-849f-8b82478226f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU score (sacreBLEU): 21.97\n"
          ]
        }
      ],
      "source": [
        "evaluate_bleu_sacre(translate, tensor_val, n_samples=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Google Drive Mounting\n",
        "\n",
        "- Mounts Google Drive to `/content/drive` for saving or loading files in Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeHua3BYh3ye",
        "outputId": "18813ac1-ab7e-4469-db32-bd6cba1290ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create Save Directory\n",
        "\n",
        "- Creates a folder in Google Drive to store model files.\n",
        "- Uses `/content/drive/MyDrive/DL/seq2seq` as the save location.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "FS77DDcdh-Yu"
      },
      "outputs": [],
      "source": [
        "save_dir = \"/content/drive/MyDrive/DL/seq2seq\"\n",
        "os.makedirs(save_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Save Model Checkpoint\n",
        "\n",
        "- Saves encoder, decoder, optimizer states, vocabularies, and current epoch as a `.pt` checkpoint file in the specified Drive folder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "6vqXEuMTiD_j"
      },
      "outputs": [],
      "source": [
        "torch.save({\n",
        "    'encoder_state_dict': encoder.state_dict(),\n",
        "    'decoder_state_dict': decoder.state_dict(),\n",
        "    'enc_opt_state_dict': enc_opt.state_dict(),\n",
        "    'dec_opt_state_dict': dec_opt.state_dict(),\n",
        "    'epoch': epoch,\n",
        "    'SRC_vocab': SRC_vocab,\n",
        "    'TGT_vocab': TGT_vocab,\n",
        "}, f\"{save_dir}/vanilla_seq2seq_checkpoint.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RNN with attention "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Encoder-Decoder with Attention (Bahdanau Attention)\n",
        "\n",
        "**Encoder Architecture**\n",
        "\n",
        "- Uses a single-layer **GRU (Gated Recurrent Unit)** to process source sequences.\n",
        "- Converts source tokens to embeddings using `nn.Embedding`.\n",
        "- Produces two outputs:\n",
        "  - **Encoder Outputs**: Hidden states for all time steps (used for attention).\n",
        "  - **Final Hidden State**: Passed to the decoder to initialize generation.\n",
        "\n",
        "\n",
        "**Decoder with Bahdanau Attention**\n",
        "\n",
        "- Standard **GRU-based Decoder** enhanced with **Bahdanau Additive Attention**.\n",
        "- Decoder Steps:\n",
        "  1. Embeds current input token.\n",
        "  2. Computes attention weights based on decoder hidden state and encoder outputs.\n",
        "  3. Generates context vector by attending to relevant encoder hidden states.\n",
        "  4. Concatenates context, embedding, and passes to GRU for next prediction.\n",
        "  5. Outputs the next token prediction using a linear layer.\n",
        "\n",
        "\n",
        "#### Why Use Attention?\n",
        "\n",
        "- **Traditional Seq2Seq Limitation**: Encoder compresses the entire input sequence into a single fixed-size vector, which can cause information loss, especially for long sentences.\n",
        "- **Attention Mechanism Benefits**:\n",
        "  - Allows decoder to access all encoder hidden states dynamically.\n",
        "  - Focuses on different parts of the input at each decoding step.\n",
        "  - Improves translation quality, especially for complex or long sentences.\n",
        "  - Provides interpretability by showing which source tokens the model attends to during generation.\n",
        "\n",
        "\n",
        "#### Summary\n",
        "\n",
        "This architecture combines the strengths of RNNs for sequence modeling with attention to overcome memory bottlenecks and enhance translation accuracy.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrMNRQoyqfDo",
        "outputId": "b538a3e3-4039-4043-8b33-7c8f8a5ace32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "caution: filename not matched:  -y\n",
            "Epoch 1, Train Loss: 2.5560, Val Loss: 1.8949\n",
            "Epoch 2, Train Loss: 1.4920, Val Loss: 1.5498\n",
            "Epoch 3, Train Loss: 0.9882, Val Loss: 1.3833\n",
            "Epoch 4, Train Loss: 0.6956, Val Loss: 1.3136\n",
            "Epoch 5, Train Loss: 0.5241, Val Loss: 1.3160\n",
            "Epoch 6, Train Loss: 0.4276, Val Loss: 1.3146\n",
            "Epoch 7, Train Loss: 0.3674, Val Loss: 1.3376\n",
            "Epoch 8, Train Loss: 0.3263, Val Loss: 1.3591\n",
            "Epoch 9, Train Loss: 0.3060, Val Loss: 1.3743\n",
            "Epoch 10, Train Loss: 0.2885, Val Loss: 1.4148\n",
            "Epoch 11, Train Loss: 0.2751, Val Loss: 1.3960\n",
            "Epoch 12, Train Loss: 0.2743, Val Loss: 1.4070\n",
            "Epoch 13, Train Loss: 0.2618, Val Loss: 1.4179\n",
            "Epoch 14, Train Loss: 0.2560, Val Loss: 1.4103\n",
            "Epoch 15, Train Loss: 0.2478, Val Loss: 1.4250\n"
          ]
        }
      ],
      "source": [
        "import os, re, unicodedata, random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "# Normalize and tokenize\n",
        "\n",
        "def normalize(s):\n",
        "    s = unicodedata.normalize('NFD', s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s.strip()\n",
        "\n",
        "# Download\n",
        "!wget -q http://www.manythings.org/anki/fra-eng.zip\n",
        "!unzip -q fra-eng.zip -y\n",
        "\n",
        "with open(\"fra.txt\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().strip().split('\\n')\n",
        "pairs = [[normalize(s) for s in l.split('\\t')[:2]] for l in lines][:10000]\n",
        "\n",
        "# Build vocab\n",
        "\n",
        "def tokenize(s): return s.split()\n",
        "\n",
        "def build_vocab(pairs, idx):\n",
        "    counter = Counter()\n",
        "    for pair in pairs:\n",
        "        counter.update(tokenize(pair[idx]))\n",
        "    vocab = {'<pad>':0, '<sos>':1, '<eos>':2, '<unk>':3}\n",
        "    for word in counter:\n",
        "        vocab[word] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "SRC_vocab = build_vocab(pairs, 0)\n",
        "TGT_vocab = build_vocab(pairs, 1)\n",
        "SRC_itos = {i:s for s,i in SRC_vocab.items()}\n",
        "TGT_itos = {i:s for s,i in TGT_vocab.items()}\n",
        "\n",
        "# Encode\n",
        "\n",
        "def encode(sentence, vocab):\n",
        "    return [vocab.get(w, vocab['<unk>']) for w in tokenize(sentence)]\n",
        "\n",
        "def tensorify(pair):\n",
        "    src = torch.tensor([SRC_vocab['<sos>']] + encode(pair[0], SRC_vocab) + [SRC_vocab['<eos>']], dtype=torch.long)\n",
        "    tgt = torch.tensor([TGT_vocab['<sos>']] + encode(pair[1], TGT_vocab) + [TGT_vocab['<eos>']], dtype=torch.long)\n",
        "    return src, tgt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_pairs, val_pairs = train_test_split(pairs, test_size=0.1, random_state=42)\n",
        "tensor_train = [tensorify(p) for p in train_pairs]\n",
        "tensor_val = [tensorify(p) for p in val_pairs]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src, tgt = zip(*batch)\n",
        "    return pad_sequence(src, padding_value=SRC_vocab['<pad>']), pad_sequence(tgt, padding_value=TGT_vocab['<pad>'])\n",
        "\n",
        "train_loader = DataLoader(tensor_train, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(tensor_val, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src)\n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "        return outputs, hidden\n",
        "\n",
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.Linear(enc_hid_dim + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Parameter(torch.rand(dec_hid_dim))\n",
        "\n",
        "    def forward(self, decoder_hidden, encoder_outputs):\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        decoder_hidden = decoder_hidden.repeat(src_len, 1, 1)\n",
        "        energy = torch.tanh(self.attn(torch.cat((decoder_hidden, encoder_outputs), dim=2)))\n",
        "        energy = energy.permute(1, 2, 0)\n",
        "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
        "        attention = torch.bmm(v, energy).squeeze(1)\n",
        "        return torch.softmax(attention, dim=1)\n",
        "\n",
        "class AttnDecoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, attention):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim + enc_hid_dim, dec_hid_dim)\n",
        "        self.fc_out = nn.Linear(emb_dim + enc_hid_dim + dec_hid_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        a = self.attention(hidden, encoder_outputs).unsqueeze(1)\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        context = torch.bmm(a, encoder_outputs).permute(1, 0, 2)\n",
        "        rnn_input = torch.cat((embedded, context), dim=2)\n",
        "        output, hidden = self.rnn(rnn_input, hidden)\n",
        "        output = self.fc_out(torch.cat((output.squeeze(0), context.squeeze(0), embedded.squeeze(0)), dim=1))\n",
        "        return output, hidden\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "ENC_EMB_DIM = 128\n",
        "DEC_EMB_DIM = 128\n",
        "HID_DIM = 256\n",
        "\n",
        "attn = BahdanauAttention(HID_DIM, HID_DIM)\n",
        "encoder = Encoder(len(SRC_vocab), ENC_EMB_DIM, HID_DIM).to(device)\n",
        "attn_decoder = AttnDecoder(len(TGT_vocab), DEC_EMB_DIM, HID_DIM, HID_DIM, attn).to(device)\n",
        "\n",
        "enc_opt = optim.Adam(encoder.parameters(), lr=0.001)\n",
        "dec_opt = optim.Adam(attn_decoder.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TGT_vocab['<pad>'])\n",
        "\n",
        "\n",
        "def train_one_epoch():\n",
        "    encoder.train()\n",
        "    attn_decoder.train()\n",
        "    total_loss = 0\n",
        "    for src, tgt in train_loader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        enc_opt.zero_grad()\n",
        "        dec_opt.zero_grad()\n",
        "        encoder_outputs, hidden = encoder(src)\n",
        "        input_dec = tgt[0, :]\n",
        "        loss = 0\n",
        "        for t in range(1, tgt.shape[0]):\n",
        "            output, hidden = attn_decoder(input_dec, hidden, encoder_outputs)\n",
        "            loss += criterion(output, tgt[t])\n",
        "            input_dec = tgt[t]\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(encoder.parameters(), 1)\n",
        "        torch.nn.utils.clip_grad_norm_(attn_decoder.parameters(), 1)\n",
        "        enc_opt.step()\n",
        "        dec_opt.step()\n",
        "        total_loss += loss.item() / (tgt.shape[0] - 1)\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def evaluate_val():\n",
        "    encoder.eval()\n",
        "    attn_decoder.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in val_loader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            encoder_outputs, hidden = encoder(src)\n",
        "            input_dec = tgt[0, :]\n",
        "            loss = 0\n",
        "            for t in range(1, tgt.shape[0]):\n",
        "                output, hidden = attn_decoder(input_dec, hidden, encoder_outputs)\n",
        "                loss += criterion(output, tgt[t])\n",
        "                input_dec = tgt[t]\n",
        "            total_loss += loss.item() / (tgt.shape[0] - 1)\n",
        "    return total_loss / len(val_loader)\n",
        "\n",
        "\n",
        "for epoch in range(1, 16):\n",
        "    train_loss = train_one_epoch()\n",
        "    val_loss = evaluate_val()\n",
        "    print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Translation with Attention\n",
        "\n",
        "- `translate`: Generates translated output using the trained encoder-decoder with attention.\n",
        "- Computes attention at each decoding step to focus on relevant source tokens.\n",
        "- Uses greedy decoding until `<eos>` token or maximum length is reached.\n",
        "- Returns the generated target sentence as text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "JfcDXjy_seyr"
      },
      "outputs": [],
      "source": [
        "def translate(sentence, max_len=30):\n",
        "    encoder.eval()\n",
        "    attn_decoder.eval()\n",
        "    with torch.no_grad():\n",
        "        src_tensor = torch.tensor([SRC_vocab['<sos>']] + encode(sentence, SRC_vocab) + [SRC_vocab['<eos>']], dtype=torch.long).unsqueeze(1).to(device)\n",
        "        encoder_outputs, hidden = encoder(src_tensor)\n",
        "        input_token = torch.tensor([TGT_vocab['<sos>']], dtype=torch.long).to(device)\n",
        "        outputs = []\n",
        "        for _ in range(max_len):\n",
        "            output, hidden = attn_decoder(input_token, hidden, encoder_outputs)\n",
        "            pred_token = output.argmax(1).item()\n",
        "            if pred_token == TGT_vocab['<eos>']:\n",
        "                break\n",
        "            outputs.append(TGT_itos[pred_token])\n",
        "            input_token = torch.tensor([pred_token], dtype=torch.long).to(device)\n",
        "    return ' '.join(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### BLEU Score Evaluation (With Attention)\n",
        "\n",
        "- Evaluates translation quality of the attention-based model on 100 validation samples.\n",
        "- Uses `sacrebleu` to calculate BLEU score and assess translation accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KndLZdlnsfQ-",
        "outputId": "df386a10-84f0-4d5e-b973-7b8a3de243a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU score (sacreBLEU): 24.60\n"
          ]
        }
      ],
      "source": [
        "evaluate_bleu_sacre(translate, tensor_val, n_samples=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Save Attention Seq2Seq Checkpoint\n",
        "\n",
        "- Saves encoder, decoder with attention, optimizer states, vocabularies, index mappings, and epoch to a `.pth` file for later use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "u1Xgmebms3c_"
      },
      "outputs": [],
      "source": [
        "checkpoint = {\n",
        "    'encoder_state_dict': encoder.state_dict(),\n",
        "    'decoder_state_dict': attn_decoder.state_dict(),\n",
        "    'encoder_optimizer_state_dict': enc_opt.state_dict(),\n",
        "    'decoder_optimizer_state_dict': dec_opt.state_dict(),\n",
        "    'SRC_vocab': SRC_vocab,\n",
        "    'TGT_vocab': TGT_vocab,\n",
        "    'SRC_itos': SRC_itos,\n",
        "    'TGT_itos': TGT_itos,\n",
        "    'epoch': epoch\n",
        "}\n",
        "\n",
        "torch.save(checkpoint, f\"{save_dir}/attention_seq2seq_checkpoint.pth\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Final Results: Text Translation with and without Attention\n",
        "\n",
        "**BLEU Score Explanation**\n",
        "\n",
        "- **BLEU (Bilingual Evaluation Understudy)** is a standard metric to evaluate machine translation quality.\n",
        "- It compares machine-generated translations with human references using n-gram overlap and brevity penalties.\n",
        "- BLEU ranges from **0 to 100**; higher scores mean better translation accuracy.\n",
        "\n",
        "\n",
        "**Model Performance Comparison**\n",
        "\n",
        "| Model                          | BLEU Score | Ideal BLEU (Toy Dataset) |\n",
        "|--------------------------------|------------|--------------------------|\n",
        "| Vanilla Seq2Seq (No Attention) | **21.97**  | 20 - 25 (Expected range) |\n",
        "| Seq2Seq with Bahdanau Attention | **24.60** | 23 - 27 (Improved with attention) |\n",
        "\n",
        "\n",
        "**Observations**\n",
        "\n",
        "Attention-based model improves translation quality by dynamically focusing on relevant input words.\n",
        "\n",
        "Both models achieve reasonable BLEU scores for a small-scale English-French dataset (~10k pairs).\n",
        "\n",
        "BLEU above **20** is typical for simple experiments; large real-world datasets aim for **30-40+** BLEU.\n",
        "\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "- Attention mechanism significantly enhances translation performance.\n",
        "- BLEU score improvement confirms better handling of longer or complex sentences.\n",
        "- Future work can include exploring different attention types, larger datasets, or transformer-based models.\n",
        "\n",
        "\n",
        "**References**\n",
        "\n",
        "1. **Seq2Seq (Sequence to Sequence Learning)**  \n",
        "   Sutskever, I., Vinyals, O., & Le, Q. V. (2014).  \n",
        "   *Sequence to sequence learning with neural networks.*  \n",
        "   Advances in Neural Information Processing Systems (NeurIPS), 27.  \n",
        "   [Read Paper](https://papers.nips.cc/paper_files/paper/2014/file/a14ac55a4f27472c5d894ec1c3c743d2-Paper.pdf)\n",
        "\n",
        "2. **Attention Mechanism**  \n",
        "   Bahdanau, D., Cho, K., & Bengio, Y. (2015).  \n",
        "   *Neural machine translation by jointly learning to align and translate.*  \n",
        "   International Conference on Learning Representations (ICLR).  \n",
        "   [https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)\n",
        "\n",
        "3. **Fra-Eng Dataset (ManyThings.org)**  \n",
        "   Tatoeba. (n.d.).  \n",
        "   *English–French sentence pairs from the Tatoeba Project* [Data set]. ManyThings.org.  \n",
        "   [http://www.manythings.org/anki/](http://www.manythings.org/anki/)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
