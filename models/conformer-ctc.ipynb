{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conformer + CTC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup with dependency installation and directory creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-23T17:08:20.215962Z",
     "iopub.status.busy": "2025-06-23T17:08:20.215687Z",
     "iopub.status.idle": "2025-06-23T17:08:25.437697Z",
     "shell.execute_reply": "2025-06-23T17:08:25.436860Z",
     "shell.execute_reply.started": "2025-06-23T17:08:20.215944Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: jiwer in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
      "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchaudio) (2.6.0+cu124)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchaudio) (1.3.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\n",
      "Requirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.11/dist-packages (from jiwer) (3.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchaudio) (3.0.2)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "libsndfile1 is already the newest version (1.0.31-2ubuntu0.2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 87 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!pip install torchaudio jiwer einops\n",
    "!apt-get install -y libsndfile1\n",
    "\n",
    "!mkdir -p ./data/librispeech "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and device configuration for GPU/CPU usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T18:37:58.622284Z",
     "iopub.status.busy": "2025-06-23T18:37:58.621538Z",
     "iopub.status.idle": "2025-06-23T18:37:58.626895Z",
     "shell.execute_reply": "2025-06-23T18:37:58.626054Z",
     "shell.execute_reply.started": "2025-06-23T18:37:58.622259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import time\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchaudio.transforms import MelSpectrogram, Resample\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "from jiwer import wer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Audio Preprocessing Pipeline\n",
    " Defined a pre-processing class which converts the audio files into mel-spectrogram features and also applying the simple rate normalization, mel-frequency transformation, log scaling and z-score normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T18:38:08.234667Z",
     "iopub.status.busy": "2025-06-23T18:38:08.234109Z",
     "iopub.status.idle": "2025-06-23T18:38:08.239936Z",
     "shell.execute_reply": "2025-06-23T18:38:08.239224Z",
     "shell.execute_reply.started": "2025-06-23T18:38:08.234647Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LibriSpeechPreprocessor:\n",
    "    def __init__(self, sample_rate=16000, n_mels=80, n_fft=512, hop_length=256):\n",
    "        self.mel = MelSpectrogram(sample_rate=sample_rate, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length)\n",
    "        self.resample = Resample(orig_freq=sample_rate, new_freq=sample_rate)\n",
    "\n",
    "    def __call__(self, waveform, sample_rate):\n",
    "        if sample_rate != 16000:\n",
    "            waveform = self.resample(waveform)\n",
    "        mel = self.mel(waveform)\n",
    "        mel = torch.log(torch.clamp(mel, min=1e-5))\n",
    "        mel = (mel - mel.mean()) / (mel.std() + 1e-5)\n",
    "        return mel.squeeze(0).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Dataset Classes\n",
    "\n",
    "\n",
    "Implements character-level tokenization and dataset handling for the LibriSpeech corpus.\n",
    "\n",
    "**CharTokenizer Class**:\n",
    "- **Vocabulary**: 28 characters (space + lowercase letters + apostrophe)\n",
    "- **Character-level encoding**: Each character maps to a unique integer index\n",
    "- **CTC blank token**: Index 0 serves as the CTC blank token for alignment\n",
    "- **Case normalization**: All text is converted to lowercase\n",
    "\n",
    "**LibriSpeechDataset Class**:\n",
    "- **Dataset wrapper**: Interfaces with torchaudio's LibriSpeech dataset\n",
    "- **Automatic download**: Downloads the specified LibriSpeech subset\n",
    "- **Transform application**: Applies the audio preprocessing pipeline\n",
    "- **Text normalization**: Converts text to lowercase\n",
    "\n",
    "**collate_fn Function**:\n",
    "- **Batch preparation**: Handles variable-length sequences in batches\n",
    "- **Padding**: Pads both audio features and text labels to maximum length in batch\n",
    "- **Length tracking**: Returns sequence lengths for CTC loss calculation\n",
    "- **Format**: Returns (padded_features, feature_lengths, padded_labels, label_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T18:38:14.959201Z",
     "iopub.status.busy": "2025-06-23T18:38:14.958419Z",
     "iopub.status.idle": "2025-06-23T18:38:14.970967Z",
     "shell.execute_reply": "2025-06-23T18:38:14.970262Z",
     "shell.execute_reply.started": "2025-06-23T18:38:14.959173Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CharTokenizer:\n",
    "    def __init__(self):\n",
    "        self.vocab = \" abcdefghijklmnopqrstuvwxyz'\"\n",
    "        self.char2idx = {c: i for i, c in enumerate(self.vocab)}\n",
    "        self.idx2char = {i: c for i, c in enumerate(self.vocab)}\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.char2idx.get(c, 0) for c in text.lower()]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        return ''.join([self.idx2char.get(i, '') for i in indices if i != 0])\n",
    "\n",
    "tokenizer = CharTokenizer()\n",
    "\n",
    "class LibriSpeechDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, url, transform):\n",
    "        self.dataset = torchaudio.datasets.LIBRISPEECH(root, url=url, download=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform, sr, text, *_ = self.dataset[idx]\n",
    "        return self.transform(waveform, sr), text.lower()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    specs, texts = zip(*batch)\n",
    "    specs = [s for s in specs]\n",
    "    labels = [torch.tensor(tokenizer.encode(t)) for t in texts]\n",
    "    return (\n",
    "        pad_sequence(specs, batch_first=True),\n",
    "        torch.tensor([len(s) for s in specs]),\n",
    "        pad_sequence(labels, batch_first=True),\n",
    "        torch.tensor([len(l) for l in labels]),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conformer Architecture Implementation\n",
    "\n",
    "\n",
    "**Swish Activation**:\n",
    "- **Modern activation function**: x * sigmoid(x), often performs better than ReLU\n",
    "- **Smooth gradient**: Provides better gradient flow during training\n",
    "\n",
    "**ConformerBlock Architecture**:\n",
    "The Conformer block follows the \"Sandwich\" architecture with four main components:\n",
    "\n",
    "1. **Feed-Forward Module 1 (FF1)**:\n",
    "   - Layer normalization → Linear expansion → Swish → Linear projection\n",
    "   - Expands dimension by 4x then projects back\n",
    "\n",
    "2. **Multi-Head Self-Attention**:\n",
    "   - Captures long-range dependencies in the sequence\n",
    "   - Uses layer normalization for stability\n",
    "\n",
    "3. **Convolution Module**:\n",
    "   - **Point-wise convolution**: 1x1 conv for dimension mixing\n",
    "   - **GLU (Gated Linear Unit)**: Gating mechanism for selective feature processing\n",
    "   - **Depth-wise convolution**: Captures local temporal patterns with kernel_size=15\n",
    "   - **Batch normalization**: Stabilizes training\n",
    "   - **Swish activation**: Non-linearity\n",
    "\n",
    "4. **Feed-Forward Module 2 (FF2)**:\n",
    "   - Similar to FF1 but applied after convolution\n",
    "   - Final layer normalization\n",
    "\n",
    "**ConformerCTCModel Architecture**:\n",
    "- **Frontend**: Two convolutional layers with stride 2\n",
    "- **Encoder**: Stack of 8 Conformer blocks\n",
    "- **Output layer**: Linear projection to vocabulary size for CTC\n",
    "\n",
    "**Key Design Choices**:\n",
    "- **Temporal downsampling**: Reduces sequence length for computational efficiency\n",
    "- **Residual connections**: Help with gradient flow and training stability\n",
    "- **Layer normalization**: Applied before each sub-module \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T18:38:34.759043Z",
     "iopub.status.busy": "2025-06-23T18:38:34.758765Z",
     "iopub.status.idle": "2025-06-23T18:38:34.768679Z",
     "shell.execute_reply": "2025-06-23T18:38:34.767969Z",
     "shell.execute_reply.started": "2025-06-23T18:38:34.759022Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "class ConformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, heads, kernel_size):\n",
    "        super().__init__()\n",
    "        self.ff1 = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            Swish(),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, heads, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(d_model, d_model * 2, 1),\n",
    "            nn.GLU(dim=1),\n",
    "            nn.Conv1d(d_model, d_model, kernel_size, padding=kernel_size // 2, groups=d_model),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            Swish(),\n",
    "            nn.Conv1d(d_model, d_model, 1)\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.ff2 = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            Swish(),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + 0.5 * self.ff1(x)\n",
    "        x = x + self.self_attn(x, x, x)[0]\n",
    "        x = self.ln1(x)\n",
    "        conv_input = rearrange(x, 'b t d -> b d t')\n",
    "        conv_out = self.conv(conv_input)\n",
    "        x = x + rearrange(conv_out, 'b d t -> b t d')\n",
    "        x = x + 0.5 * self.ff2(x)\n",
    "        return self.ln2(x)\n",
    "\n",
    "class ConformerCTCModel(nn.Module):\n",
    "    def __init__(self, input_dim=80, d_model=256, num_blocks=8, heads=4, vocab_size=30):\n",
    "        super().__init__()\n",
    "        self.frontend = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, d_model // 2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(d_model // 2, d_model, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.encoder = nn.Sequential(*[ConformerBlock(d_model, heads, kernel_size=15) for _ in range(num_blocks)])\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # (B, F, T)\n",
    "        x = self.frontend(x)    # (B, D, T//4)\n",
    "        x = x.permute(0, 2, 1)  # (B, T//4, D)\n",
    "        x = self.encoder(x)\n",
    "        return self.fc(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions\n",
    "\n",
    "Implements the training and evaluation loops with CTC loss and greedy decoding.\n",
    "\n",
    "**greedy_decode Function**:\n",
    "- **CTC decoding**: Removes blank tokens (index 0) and repeated consecutive tokens\n",
    "- **Simple decoding**: Uses argmax for token selection (no beam search)\n",
    "- **Text reconstruction**: Converts token indices back to readable text\n",
    "\n",
    "**train_epoch Function**:\n",
    "- **Training loop**: Iterates through batches with progress tracking\n",
    "- **CTC loss**: Uses PyTorch's CTCLoss with blank=0\n",
    "- **Gradient clipping**: Prevents exploding gradients (max norm=5.0)\n",
    "- **Learning rate scheduling**: OneCycleLR scheduler for each step\n",
    "- **Performance tracking**: Returns average loss and training time\n",
    "\n",
    "**evaluate Function**:\n",
    "- **Evaluation mode**: No gradient computation for efficiency\n",
    "- **Loss calculation**: Same CTC loss as training\n",
    "- **WER computation**: Calculates Word Error Rate using jiwer library\n",
    "- **Metrics**: Returns both loss and WER for model assessment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T18:38:42.885919Z",
     "iopub.status.busy": "2025-06-23T18:38:42.885670Z",
     "iopub.status.idle": "2025-06-23T18:38:42.893752Z",
     "shell.execute_reply": "2025-06-23T18:38:42.893087Z",
     "shell.execute_reply.started": "2025-06-23T18:38:42.885903Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def greedy_decode(preds):\n",
    "    return [\"\".join(tokenizer.decode([p for i, p in enumerate(seq) if p != 0 and (i == 0 or p != seq[i-1])])) for seq in preds]\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scheduler, criterion):\n",
    "    model.train()\n",
    "    total_loss, start = 0, time.time()\n",
    "    for x, xlen, y, ylen in tqdm(loader, desc=\"Train\"):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x).permute(1, 0, 2)  # (T, B, C)\n",
    "        loss = criterion(out, y, xlen // 4, ylen)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader), time.time() - start\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, total_wer = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, xlen, y, ylen in tqdm(loader, desc=\"Eval\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x).permute(1, 0, 2)\n",
    "            loss = criterion(out, y, xlen // 4, ylen)\n",
    "            total_loss += loss.item()\n",
    "            pred = torch.argmax(out, dim=-1).permute(1, 0)\n",
    "            hyp = greedy_decode(pred)\n",
    "            ref = [tokenizer.decode(t.cpu().numpy()) for t in y]\n",
    "            total_wer += wer(ref, hyp)\n",
    "    return total_loss / len(loader), total_wer / len(loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Execution\n",
    "\n",
    "\n",
    "**Dataset Setup**:\n",
    "- **Training data**: LibriSpeech train-clean-100 \n",
    "- **Validation split**: 90% train, 10% validation from training data\n",
    "- **Test data**: LibriSpeech test-clean \n",
    "- **Data loaders**: Batch size of 8 with custom collate function\n",
    "\n",
    "**Model Configuration**:\n",
    "- **Architecture**: ConformerCTC with default parameters\n",
    "- **Vocabulary size**: 28 characters \n",
    "\n",
    "**Training Configuration**:\n",
    "- **Optimizer**: AdamW with learning rate 3e-4\n",
    "- **Scheduler**: OneCycleLR for learning rate scheduling\n",
    "- **Loss function**: CTCLoss with blank token at index 0\n",
    "- **Training duration**: 10 epochs\n",
    "\n",
    "**Training Loop**:\n",
    "- **Epoch tracking**: Trains for 10 epochs\n",
    "- **Progress monitoring**: Prints loss and WER metrics\n",
    "- **Model checkpointing**: Saves model after each epoch\n",
    "- **Final evaluation**: Tests on held-out test set\n",
    "\n",
    "**Key Training Details**:\n",
    "- **Batch size**: 8 \n",
    "- **Learning rate**: 3e-4\n",
    "- **OneCycleLR**: Implements learning rate warmup and decay\n",
    "- **Model saving**: Checkpoints for potential fine-tuning or analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-23T18:38:49.379526Z",
     "iopub.status.busy": "2025-06-23T18:38:49.378962Z",
     "iopub.status.idle": "2025-06-23T19:54:03.711543Z",
     "shell.execute_reply": "2025-06-23T19:54:03.710816Z",
     "shell.execute_reply.started": "2025-06-23T18:38:49.379502Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3211/3211 [06:29<00:00,  8.24it/s]\n",
      "Eval: 100%|██████████| 357/357 [00:52<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5304 | Val Loss: 1.6734 | Val WER: 100.00% | Time: 389.56s\n",
      "\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3211/3211 [06:28<00:00,  8.27it/s]\n",
      "Eval: 100%|██████████| 357/357 [00:52<00:00,  6.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5694 | Val Loss: 1.5625 | Val WER: 100.00% | Time: 388.34s\n",
      "\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3211/3211 [06:29<00:00,  8.25it/s]\n",
      "Eval: 100%|██████████| 357/357 [00:57<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4488 | Val Loss: 1.4636 | Val WER: 100.00% | Time: 389.23s\n",
      "\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3211/3211 [06:28<00:00,  8.26it/s]\n",
      "Eval: 100%|██████████| 357/357 [01:00<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2988 | Val Loss: 1.3218 | Val WER: 100.00% | Time: 388.88s\n",
      "\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3211/3211 [06:27<00:00,  8.30it/s]\n",
      "Eval: 100%|██████████| 357/357 [00:57<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1646 | Val Loss: 1.1460 | Val WER: 100.00% | Time: 387.01s\n",
      "\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3211/3211 [06:27<00:00,  8.29it/s]\n",
      "Eval: 100%|██████████| 357/357 [00:59<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0554 | Val Loss: 1.0904 | Val WER: 100.00% | Time: 387.34s\n",
      "\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3211/3211 [06:27<00:00,  8.28it/s]\n",
      "Eval: 100%|██████████| 357/357 [01:00<00:00,  5.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9522 | Val Loss: 0.9866 | Val WER: 100.00% | Time: 387.90s\n",
      "\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3211/3211 [06:28<00:00,  8.26it/s]\n",
      "Eval: 100%|██████████| 357/357 [01:01<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8564 | Val Loss: 0.9451 | Val WER: 100.00% | Time: 388.75s\n",
      "\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3211/3211 [06:27<00:00,  8.28it/s]\n",
      "Eval: 100%|██████████| 357/357 [01:01<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7776 | Val Loss: 0.9115 | Val WER: 100.00% | Time: 387.92s\n",
      "\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 3211/3211 [06:27<00:00,  8.30it/s]\n",
      "Eval: 100%|██████████| 357/357 [01:01<00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7321 | Val Loss: 0.9134 | Val WER: 100.00% | Time: 387.00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Eval: 100%|██████████| 328/328 [00:44<00:00,  7.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Loss: 0.7932 | Test WER: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessor = LibriSpeechPreprocessor()\n",
    "train_dataset = LibriSpeechDataset('./data/librispeech', 'train-clean-100', preprocessor)\n",
    "train_len = int(0.9 * len(train_dataset))\n",
    "train_data, val_data = random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n",
    "test_data = LibriSpeechDataset('./data/librispeech', 'test-clean', preprocessor)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_data, batch_size=8, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=8, collate_fn=collate_fn)\n",
    "\n",
    "model = ConformerCTCModel(vocab_size=tokenizer.vocab_size).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=3e-4, steps_per_epoch=len(train_loader), epochs=10)\n",
    "criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    print(f\"\\nEpoch {epoch}\")\n",
    "    train_loss, train_time = train_epoch(model, train_loader, optimizer, scheduler, criterion)\n",
    "    val_loss, val_wer = evaluate(model, val_loader, criterion)\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val WER: {val_wer:.2%} | Time: {train_time:.2f}s\")\n",
    "    torch.save(model.state_dict(), f\"conformer_ctc_epoch{epoch}.pth\")\n",
    "\n",
    "test_loss, test_wer = evaluate(model, test_loader, criterion)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f} | Test WER: {test_wer:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "1. **Loss Convergence**: Training loss decreases from 1.53 to 0.73 over 10 epochs\n",
    "2. **Validation Performance**: Validation loss follows similar trend\n",
    "3. **WER Issue**: 100% WER suggests potential issues with:\n",
    "   - CTC alignment not converging properly\n",
    "   - Learning rate or model capacity issues\n",
    "   - Need for longer training or different hyperparameters\n",
    "\n",
    "4. **Training Time**: ~6.5 minutes per epoch on GPU\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
